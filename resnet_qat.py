# -*- coding: utf-8 -*-
"""Resnet_qat.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RUWQafKfCNom1yrW-8hoyvPqjtiu82x0
"""

!pip install -q tensorflow-model-optimization

!pip install -U numpy

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)

import pathlib

import os
import tensorflow as tf
from tensorflow import keras
import tensorflow_model_optimization as tfmot
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, applications
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint
from sklearn.model_selection import train_test_split
import math

best_model_path = '/content/drive/MyDrive/Colab Notebooks/Resnet model/resetcifar_best_model.h5'
best_model = tf.keras.models.load_model(best_model_path)

# tflite_models_dir = pathlib.Path("/content/drive/MyDrive/Colab Notebooks/Resnet model")
# base_tflite_model_file = tflite_models_dir/"resnetcifar_base.tflite"
# base_tflite_model = tf.keras.models.load_model(base_tflite_model_file)

# Load and preprocess the dataset
(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()
train_images = train_images.astype('float32') / 255.0
test_images = test_images.astype('float32') / 255.0
train_labels = to_categorical(train_labels, 10)
test_labels = to_categorical(test_labels, 10)

# Split the training data into training and validation sets
train_images, val_images, train_labels, val_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)

quantize_model = tfmot.quantization.keras.quantize_model
# q_aware stands for for quantization aware.
q_aware_model = quantize_model(best_model)
# `quantize_model` requires a recompile.
opt = Adam(learning_rate=1e-4)
q_aware_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

#q_aware_model.summary()

q_aware_model.fit(train_images, train_labels, batch_size=128, epochs=5, validation_data=(val_images, val_labels))

# quantized model with int8 weights and uint8 activations.
converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
qat_tflite_model = converter.convert()

tflite_models_dir = pathlib.Path("/content/drive/MyDrive/Colab Notebooks/Resnet model")
tflite_models_dir.mkdir(exist_ok=True, parents=True)

qat_tflite_model_file = tflite_models_dir/"resnetcifar_qat.tflite"
qat_tflite_model_file.write_bytes(qat_tflite_model)

# Create float TFLite model.
qat_float_converter = tf.lite.TFLiteConverter.from_keras_model(best_model)
qat_float_tflite_model = float_converter.convert()

qat_float_tflite_model_file = tflite_models_dir/"resnetcifar_float_qat.tflite"
qat_float_tflite_model_file.write_bytes(qat_float_tflite_model)

interpreter = tf.lite.Interpreter(model_path=str(qat_tflite_model_file))
interpreter.allocate_tensors()

import time

# Function to preprocess input data for TFLite model
def preprocess_input_data(images):
    input_data = np.expand_dims(images, axis=0)
    input_data = np.float32(input_data)
    return input_data

# Function to run inference using the TFLite model
def run_inference(interpreter, input_data):
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    interpreter.set_tensor(input_details[0]['index'], input_data)
    interpreter.invoke()
    
    output_data = interpreter.get_tensor(output_details[0]['index'])
    return output_data

# Evaluate the TFLite model on the test dataset
start_time = time.time()
correct_predictions = 0
total_predictions = len(test_images)

for i in range(total_predictions):
    input_data = preprocess_input_data(test_images[i])
    output_data = run_inference(interpreter, input_data)
    
    predicted_label = np.argmax(output_data)
    true_label = np.argmax(test_labels[i])
    
    if predicted_label == true_label:
        correct_predictions += 1

# Calculate the accuracy - remains same
accuracy = correct_predictions / total_predictions

end_time = time.time()

print("QAT TFLite model accuracy on CIFAR-10 dataset: {:.2f}%".format(accuracy * 100))

duration = end_time - start_time
hours = duration // 3600
minutes = (duration - (hours * 3600)) // 60
seconds = duration - ((hours * 3600) + (minutes * 60))
inference_msg = f'QAT TFlite inference time: {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'
print(inference_msg)

interpreter = tf.lite.Interpreter(model_path=str(qat_float_tflite_model_file))
interpreter.allocate_tensors()

# Evaluate the base model on the test dataset
start_time = time.time()
correct_predictions = 0
total_predictions = len(test_images)

for i in range(total_predictions):
    input_data = preprocess_input_data(test_images[i])
    output_data = run_inference(interpreter, input_data)
    
    predicted_label = np.argmax(output_data)
    true_label = np.argmax(test_labels[i])
    
    if predicted_label == true_label:
        correct_predictions += 1

# Calculate the accuracy for TFLite model
accuracy = correct_predictions / total_predictions
end_time = time.time()
print("Float QAT model accuracy on CIFAR-10 dataset after quantization: {:.2f}%".format(accuracy * 100))

duration = end_time - start_time
hours = duration // 3600
minutes = (duration - (hours * 3600)) // 60
seconds = duration - ((hours * 3600) + (minutes * 60))
inference_msg = f'Float QAT model inference time: {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'
print(inference_msg)

interpreter = tf.lite.Interpreter(model_path=str(base_tflite_model_file))
interpreter.allocate_tensors()

# Evaluate the base model on the test dataset
start_time = time.time()
correct_predictions = 0
total_predictions = len(test_images)

for i in range(total_predictions):
    input_data = preprocess_input_data(test_images[i])
    output_data = run_inference(interpreter, input_data)
    
    predicted_label = np.argmax(output_data)
    true_label = np.argmax(test_labels[i])
    
    if predicted_label == true_label:
        correct_predictions += 1

# Calculate the accuracy for TFLite model
accuracy = correct_predictions / total_predictions
end_time = time.time()
print("Base model accuracy on CIFAR-10 dataset after quantization: {:.2f}%".format(accuracy * 100))

duration = end_time - start_time
hours = duration // 3600
minutes = (duration - (hours * 3600)) // 60
seconds = duration - ((hours * 3600) + (minutes * 60))
inference_msg = f'Base model inference time: {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'
print(inference_msg)

import tempfile
import os
def get_gzipped_model_size(file):
  # Returns size of gzipped model, in bytes.
  import zipfile

  _, zipped_file = tempfile.mkstemp('.zip')
  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:
    f.write(file)

  return os.path.getsize(zipped_file)

print("Size of gzipped baseline model: %.2f bytes" % (get_gzipped_model_size(best_model)))
print("Size of gzipped baseline TFLite model: %.2f bytes" % (get_gzipped_model_size(base_tflite_model_file)))
print("Size of gzipped pruned TFLite model: %.2f bytes" % (get_gzipped_model_size(qat_tflite_model_file)))